<?xml version="1.0" encoding="UTF-8"?>
<!-- note
-->
<data>
    <urls>
        <url name="absPath" type="dir">/media/mtirico/DATA/project/learnDyNet/learnDyNet</url>
        <url name="scenarios" type="dir">scenarios</url>  <!-- where are stored all scenarios-->
        <url name="scenario" type="dir">grid_01</url>           <!-- where it is stored scenario to study-->
        <url name="tmp" type="dir">.tmp</url>
        <url name="resources" type="dir"> resources </url>
        <url name="states" type="file">states.xml</url> <!-- the xml with all states and links -->
        <url name="outputs" type="dir">outputs</url>  <!-- where are stored all scenarios-->
    </urls>

    <simulation>
        <parameter name="numStep">5000  </parameter> <!-- 1 means we do only the first sim (named sim-0000) -->
        <parameter name="initState">5</parameter> <!-- random or fix a kind of state. if random, do not need set value, otherwise set init state type -->
        <parameter name="initStateMode">random</parameter> <!-- or same -->
        <parameter name="seedInitStateMode">2</parameter>
    </simulation>

    <mobility>
        <parameter name="typeInitIndividuals">random</parameter>
        <parameter name="percentIndividuals">10</parameter>
        <parameter name="multipleIndividualsOverVertex">True</parameter>
        <parameter name="alpha_m">0.8,0.5,0.1</parameter> <!-- that represent the political decision. this value is global. Vals are for walk,bike and car respectively  -->
        <parameter name="typeMobilityModel">discreteModeChoice</parameter>
        <parameter name="val_weight_out">0</parameter>
        <parameter name="cost_walk">0.1</parameter>
        <parameter name="cost_bike">0.5</parameter>
        <parameter name="cost_car">0.9</parameter>
        <parameter name="modes">walk,bike,car </parameter>
        <parameter name="directions">from-to,to-from</parameter>
        <parameter name="maxNumberOfEdges">2</parameter>
        <parameter name="rewardNoTripFunded">-1</parameter>
    </mobility>

    <network>
        <parameter name="typeNetwork" type="val">grid</parameter>
        <parameter name="dimension_x" type="grid">2</parameter>
        <parameter name="dimension_y" type="grid">1</parameter>
        <parameter name="dist_x" type="grid">2</parameter>
        <parameter name="dist_y" type="grid">3</parameter>
    </network>


    <learning>
        <parameter name="typelearning">qlearning</parameter>   <!-- options: qlearning,random (does not work)-->
        <parameter name="randomseed">2</parameter>          <!-- to use when you use the random type of learning. If empty, no seed are fixed -->
        <parameter name="learning_gamma">0.5</parameter>             <!-- discount factor. High values mean agent looking for reward in long time -->
        <parameter name="epsilonmin">0.1</parameter>        <!-- minimum probability of exploration (e-greedy policy). High values of epsilon means more exploration -->
        <parameter name="learning_alpha">0.1</parameter>             <!-- learning rate. High values mean more learning. -->
        <parameter name="roundqvalue"> 3 </parameter>       <!-- round the values of scores to put in qtable -->
        <parameter name="rewardNoScore"> -100 </parameter>  <!-- in the case of the no reward (score) are available at the step x, we give a fixed reward -->
        <parameter name="rdSelectAction">3</parameter>
        <!--
        <parameter name="seedRandomAction">3</parameter>
        <parameter name="seedRandomState">6</parameter>
        <parameter name="seedRandomMode">3</parameter>
        <parameter name="seedRandomDirection">4</parameter>
        -->
        <parameter name="actionModeSelection">greedy</parameter>  <!-- random or greedy -->
    </learning>

</data>
