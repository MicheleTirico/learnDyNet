<?xml version="1.0" encoding="UTF-8"?>
<!-- note
-->
<data>
    <urls>
        <url name="absPath" type="dir">/media/mtirico/DATA/project/learnDyNet/learnDyNet</url>
        <url name="scenarios" type="dir">scenarios</url>  <!-- where are stored all scenarios-->
        <url name="scenario" type="dir">grid_01</url>           <!-- where it is stored scenario to study-->
        <url name="tmp" type="dir">.tmp</url>
        <url name="resources" type="dir"> resources </url>
        <url name="states" type="file">states.xml</url> <!-- the xml with all states and links -->
        <url name="outputs" type="dir">outputs</url>  <!-- where are stored all scenarios-->

    </urls>

    <simulation>
        <parameter name="numsim">10  </parameter> <!-- 1 means we do only the first sim (named sim-0000) -->
        <parameter name="initState"> w1</parameter> <!-- random or fix a kind of state. if random, do not need set value, otherwise set init state type -->
        <parameter name="initStateMode">random</parameter> <!-- or same -->
        <parameter name="seedInitStateMode">1</parameter>
    </simulation>

    <mobility>
        <parameter name="typeInitIndividuals">random</parameter>
        <parameter name="percentIndividuals">0.8</parameter>
        <parameter name="alpha">0.5</parameter>
        <parameter name="typeMobilityModel">discreteModeChoice</parameter>
        <parameter name="val_weigth_out">0</parameter>
    </mobility>

    <network>
        <parameter name="typeNetwork" type="val">grid</parameter>
        <parameter name="dimension_x" type="grid">2</parameter>
        <parameter name="dimension_y" type="grid">2</parameter>
        <parameter name="dist_x" type="grid">2</parameter>
        <parameter name="dist_y" type="grid">3</parameter>
    </network>


    <learning>
        <parameter name="typelearning">qtable</parameter>   <!-- options: qtable,random (does not work)-->
        <parameter name="randomseed">2</parameter>          <!-- to use when you use the random type of learning. If empty, no seed are fixed -->
        <parameter name="gamma">0.5</parameter>             <!-- discount factor. High values mean agent looking for reward in long time -->
        <parameter name="epsilonmin">0.1</parameter>        <!-- minimum probability of exploration (e-greedy policy). High values of epsilon means more exploration -->
        <parameter name="alpha">0.1</parameter>             <!-- learning rate. High values mean more learning. -->
        <parameter name="roundqvalue"> 3 </parameter>       <!-- round the values of scores to put in qtable -->
        <parameter name="rewardNoScore"> -100 </parameter>  <!-- in the case of the no reward (score) are available at the step x, we give a fixed reward -->
    </learning>

</data>
